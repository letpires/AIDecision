# üéØ AI Job Matching Platform

**Plataforma inteligente de recrutamento** que utiliza IA para analisar perfis de candidatos, conduzir entrevistas t√©cnicas simuladas e gerar recomenda√ß√µes autom√°ticas com envio de notifica√ß√µes via Telegram para recrutadores.

---

## üìå Vis√£o Geral do Projeto

Este projeto tem como objetivo otimizar o processo de recrutamento utilizando Intelig√™ncia Artificial para realizar entrevistas simuladas com candidatos. A aplica√ß√£o analisa o curr√≠culo, conduz uma conversa interativa baseada na vaga escolhida e entrega uma avalia√ß√£o com base em crit√©rios t√©cnicos e comportamentais.

---

## üöÄ Funcionalidades

- üìã Visualiza√ß√£o de vagas cadastradas
- üßë Cria√ß√£o de perfil de candidato com LinkedIn e GitHub (Futuro)
- üìÑ Upload e parsing autom√°tico de curr√≠culos (PDF)
- üìù Previs√£o/score usando modelo de classifi√ß√£o com base nos dados da empresa
- ü§ñ Entrevista t√©cnica com IA (OpenAI)
- üìä Avalia√ß√£o com pontua√ß√£o, pontos fortes e sugest√µes de melhoria
- üì≤ Notifica√ß√£o autom√°tica ao recrutador via Telegram
- üìà Monitoramento de m√©tricas com Prometheus e Grafana
- üß™ Testes unit√°rios e funcionais para garantir qualidade

---

## Modelo de classifica√ß√£o

O processo de desenvolvimento do modelo de classifica√ß√£o seguiu as seguintes etapas:

### 1. Coleta e Prepara√ß√£o dos Dados
- Coleta de arquivos JSON contendo dados hist√≥ricos da empresa
- Merge dos dados em um √∫nico DataFrame utilizando o notebook `merge.ipynb`
- An√°lise explorat√≥ria e tratamento dos dados no notebook `eda.ipynb`

### 2. Pr√©-processamento dos Dados

O pr√©-processamento dos dados textuais foi um dos principais desafios do projeto, especialmente devido √† variedade e √† qualidade das informa√ß√µes presentes nos campos de texto dos curr√≠culos e das vagas. Abaixo, detalhamos a l√≥gica adotada:

#### Consolida√ß√£o das Informa√ß√µes Textuais
- Algumas colunas como `cv_pt` (curr√≠culo em portugu√™s) podem estar nulas, enquanto outras colunas possuem informa√ß√µes relevantes do candidato.
- Concatenamos todas as colunas de texto relevantes para o candidato em uma √∫nica coluna chamada `info_candidato` e, para a vaga, em `info_vaga`. Isso garante que toda informa√ß√£o √∫til seja considerada, mesmo que esteja dispersa em diferentes campos.

#### Limpeza e Normaliza√ß√£o
- Remo√ß√£o de stopwords, acentos e caracteres especiais.
- Padroniza√ß√£o para caixa baixa (lowercase).
- Tokeniza√ß√£o dos textos.

#### Extra√ß√£o de Palavras-chave e Categorias
- A ideia inicial era utilizar IA (GPT) para analisar a descri√ß√£o da vaga e gerar automaticamente uma lista de palavras-chave e categorias relevantes, conforme exemplo abaixo:

| VAGA                   | DESCRICAO                                                                 | PALAVRAS_CHAVE                                      | CATEGORIAS                                                        |
|------------------------|---------------------------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------------------|
| ANALISTA DE DADOS JR   | Profissional com conhecimento em Power BI, Looker, Excel, an√°lise explorat√≥ria | power bi, looker, excel, analise exploratoria, ...  | {backend: null, frontend: null, dados_bi: [power bi, looker]}     |
| ANALISTA DE SISTEMAS JR| Respons√°vel por manuten√ß√£o de sistemas, SQL, l√≥gica de programa√ß√£o, UML   | sql, logica de programacao, uml, ...                | {backend: [sql, l√≥gica de programa√ß√£o], sistemas: [uml]}          |
| ANALISTA DE SISTEMAS SR| Experi√™ncia em modelagem, integra√ß√£o de APIs, lideran√ßa de projetos        | api, arquitetura, modelagem, lideranca, ...         | {backend: [api, arquitetura de software], sistemas: [modelagem]}  |

- Por restri√ß√£o de cr√©ditos na OpenAI, n√£o foi poss√≠vel usar o GPT para todo o processamento de texto.
- Optamos por definir categorias fixas e regras para extra√ß√£o de palavras-chave, utilizando dicion√°rios e listas pr√©-definidas.

#### Matching de Habilidades
- A partir das palavras-chave/categorias extra√≠das da vaga, comparamos com as informa√ß√µes do candidato (`info_candidato`) para calcular o grau de ader√™ncia (match) entre o perfil e a vaga.

Essa abordagem permitiu criar uma base s√≥lida para o modelo de classifica√ß√£o, mesmo com limita√ß√µes de uso de IA generativa.

### 3. Desenvolvimento do Modelo
- Separa√ß√£o dos dados em treino e teste
- Treinamento de diferentes algoritmos de classifica√ß√£o
- Valida√ß√£o cruzada para avalia√ß√£o do desempenho
- Sele√ß√£o do melhor modelo baseado em m√©tricas de avalia√ß√£o

### 4. Implementa√ß√£o
- Integra√ß√£o do modelo com a aplica√ß√£o principal
- Gera√ß√£o de scores para novos candidatos
- Monitoramento cont√≠nuo do desempenho do modelo

---
## üõ†Ô∏è Instala√ß√£o Local

```bash
git clone <repository-url>
cd <repository-name>

python -m venv venv
source venv/bin/activate         # ou venv\Scripts\activate no Windows

pip install -r requirements.txt
```

Crie um arquivo `.env` com:

```env
OPENAI_API_KEY=your_openai_api_key
TELEGRAM_BOT_TOKEN=your_telegram_bot_token
TELEGRAM_CHAT_ID=your_telegram_chat_id
```

Crie os diret√≥rios necess√°rios:

```bash
mkdir -p uploads dados logs
```

---

## ‚ñ∂Ô∏è Executando a Aplica√ß√£o Localmente

```bash
streamlit run app/main.py
```

Abra [http://localhost:8501](http://localhost:8501) no navegador.

---

## üß™ Executando Testes Unit√°rios e Funcionais

### Testes Unit√°rios

Executam valida√ß√µes de fun√ß√µes isoladas, como:
- An√°lise de curr√≠culo (ResumeParser)
- Avalia√ß√£o de resposta (InterviewAgent)

```bash
pytest tests/unit/
```

### Testes Funcionais

Simulam o fluxo completo da aplica√ß√£o, incluindo:
- Escolha de vaga
- Preenchimento do perfil
- Entrevista e resultado final

```bash
pytest tests/functional/
```

---

## üìä Monitoramento com Prometheus + Grafana

1. A aplica√ß√£o exp√µe m√©tricas em http://localhost:9000/metrics
2. Use docker-compose up -d para subir Prometheus e Grafana

Acesse:
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000 (admin/admin)

Importe o dashboard JSON:
- `monitoring/monitoring_dashboard.json`

---

## üê≥ Dockerfile (localizado em app/Dockerfile)

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY . /app

RUN apt-get update && apt-get install -y build-essential && \
    pip install --no-cache-dir -r requirements.txt

ENV STREAMLIT_SERVER_HEADLESS=true
ENV STREAMLIT_SERVER_PORT=8501

EXPOSE 8501
CMD ["streamlit", "run", "main.py"]
```

---

## üê≥ Docker Compose (na raiz)

```yaml
version: '3.7'

services:
  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: ai-job-matcher-app
    ports:
      - "8501:8501"
    env_file:
      - .env
    volumes:
      - ./uploads:/app/uploads
      - ./dados:/app/dados
      - ./logs:/app/logs
    restart: always

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    restart: always

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    depends_on:
      - prometheus
    restart: always

volumes:
  grafana-storage:
```

---

## ‚ñ∂Ô∏è Executando com Docker Compose

```bash
docker compose up --build -d         # Subir os servi√ßos
docker compose logs -f               # Acompanhar logs
docker compose down                  # Encerrar tudo
```

---
## ‚òÅÔ∏è Acessar os Servi√ßos

| Servi√ßo    | URL                                            | Usu√°rio | Senha          |
| ---------- | ---------------------------------------------- | ------- | -------------- |
| Streamlit  | [http://localhost:8501](http://localhost:8501) | ‚Äî       | ‚Äî              |
| Prometheus | [http://localhost:9090](http://localhost:9090) | ‚Äî       | ‚Äî              |
| Grafana    | [http://localhost:3000](http://localhost:3000) | admin   | admin (padr√£o) |


---
## üöÄ Importando o Dashboard no Grafana

1. Baixe o arquivo JSON do dashboard:

```bash
wget -O monitoring/ai_job_matcher_dashboard.json \
  https://<seu-repo>/monitoring/monitoring_dashboard.json
```

2. Acesse o Grafana em http://localhost:3000

3. No menu lateral, clique em "+" ‚Üí Import

4. No campo "Upload JSON file or Grafana.com Dashboard", selecione monitoring/monitoring_dashboard.json

5. Garanta que o campo Name seja preenchido automaticamente como "Monitoramento - AI Job Matcher"

6. elecione a Data source chamada Prometheus

7. Clique em Import

Voc√™ ver√° agora:

Total de Entrevistas (stat panel)
Acur√°cia (gauge panel)
M√©dia da Pontua√ß√£o Geral (time series)
M√©dia da Pontua√ß√£o T√©cnica (time series)
M√©dia da Pontua√ß√£o de Comunica√ß√£o (time series)


---
## **Deploy na Nuvem**

### **Deploy Usando AWS Elastic Beanstalk**

1. **Pr√©-requisitos**:
   - Instale a AWS CLI e configure com suas credenciais.
   - Certifique-se de ter um reposit√≥rio ECR (Elastic Container Registry) configurado.

2. **Push da Imagem para o ECR**:
   ```bash
   aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <account_id>.dkr.ecr.<region>.amazonaws.com
   docker tag minha-api-fastapi:latest <account_id>.dkr.ecr.<region>.amazonaws.com/minha-api-fastapi:latest
   docker push <account_id>.dkr.ecr.<region>.amazonaws.com/minha-api-fastapi:latest
   ```

3. **Criar a Aplica√ß√£o no Elastic Beanstalk**:
   - Acesse o console da AWS e v√° at√© o Elastic Beanstalk.
   - Crie uma nova aplica√ß√£o com o nome desejado.
   - Escolha a plataforma Docker e forne√ßa o URI da imagem do ECR.

4. **Configurar o Ambiente**:
   - Configure a porta 8000 no Elastic Beanstalk.
   - Fa√ßa o deploy e aguarde a inicializa√ß√£o.

5. **Acessar o Endpoint da Aplica√ß√£o**:
   O Elastic Beanstalk fornecer√° um dom√≠nio onde sua aplica√ß√£o estar√° acess√≠vel.

### **Deploy Usando uma Conta RENDER(Gratuito)**

1. Crie uma conta no Render

2. Crie um novo servi√ßo Web
  
3. Escolha "Docker" como op√ß√£o de deploy
  
4. Forne√ßa o link do reposit√≥rio do seu projeto (GitHub/GitLab)

5. Configure vari√°veis de ambiente e publique!

**LINK da Nossa API na NUVEM**:
 ```
   https://aidecision.onrender.com
   ```
Obs: O primeiro acesso pode demorar um pouco pois o container fica em sleeping (Stateless) para economizar recurso e inicia quando √© chamado.
---


---

## üìö Como Usar

1. Escolha uma vaga
2. Preencha seu perfil
3. Participe da entrevista com IA
4. Receba a avalia√ß√£o
5. Recrutador √© notificado

---

## üß± Arquitetura

| Componente     | Tecnologia              |
|----------------|--------------------------|
| Interface      | Streamlit                |
| IA Entrevista  | OpenAI GPT (via SDK)     |
| Parsing de CV  | PyMuPDF / PDFMiner       |
| Notifica√ß√£o    | Telegram Bot API         |
| M√©tricas       | Prometheus + Grafana     |
| Armazenamento  | Sistema de arquivos local|

---

## ü§ù Contribuindo

1. Fork o reposit√≥rio
2. Crie um branch `feature/nome-da-feature`
3. Fa√ßa commits claros
4. Abra um Pull Request

---

## üìÑ Licen√ßa

MIT License
